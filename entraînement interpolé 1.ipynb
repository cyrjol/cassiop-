{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"81G1KdjZEBsR","executionInfo":{"status":"ok","timestamp":1654766861621,"user_tz":-120,"elapsed":19544,"user":{"displayName":"Guillaume Principato","userId":"02696761146641071970"}},"outputId":"214301f6-1ed2-4865-8c92-04d9471003a7"},"id":"81G1KdjZEBsR","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"id":"2486e586","metadata":{"id":"2486e586"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.callbacks import TensorBoard\n","from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n","\n","import numpy as np\n","import math, random\n","import matplotlib.pyplot as plt\n","\n","import pandas as pd\n","from pandas.plotting import scatter_matrix\n","import h5py, json\n","import os,time,sys\n","\n","\n","from importlib import reload\n","\n","sys.path.append('/content/drive/MyDrive/Colab Notebooks/fidle-master')\n","import fidle.pwk as pwk\n","\n","run_dir = '/content/drive/MyDrive/Colab Notebooks'\n","\n","\n","pd.set_option('display.max_rows',200)"]},{"cell_type":"code","execution_count":null,"id":"cf173b4f","metadata":{"id":"cf173b4f"},"outputs":[],"source":[" # ---- About dataset (no need to change)\n","#\n","dataset_dir      = '/content/drive/MyDrive/Colab Notebooks'                  # Enhanced dataset is very small, so ./data in a good choice :-)\n","dataset_filename = 'synop-LYSinterpole.csv'\n","schema_filename  = 'synopinterpole.json'\n","features         = ['Température','mois_de_l_annee','Humidité']\n","features_len     = len(features)\n","\n","# ---- About training (Can be changed !)\n","#\n","scale            = 1        # Percentage of dataset to be used (1=all)\n","train_prop       = .8       # Percentage for train (the rest being for the test)\n","sequence_len     = 32\n","batch_size       = 64\n","epochs           = 10\n","fit_verbosity    = 1        # 0 = silent, 1 = progress bar, 2 = one line per epoch"]},{"cell_type":"code","execution_count":null,"id":"eb339294","metadata":{"id":"eb339294"},"outputs":[],"source":[" pwk.override('scale', 'train_prop', 'sequence_len', 'batch_size', 'epochs', 'fit_verbosity')"]},{"cell_type":"code","execution_count":null,"id":"be7c5181","metadata":{"id":"be7c5181"},"outputs":[],"source":["# ---- Read dataset from ./data\n","\n","df = pd.read_csv(f'{dataset_dir}/{dataset_filename}', header=0, sep=';')\n","\n","# ---- Scaling\n","\n","df = df[:int(scale*len(df))]\n","train_len=int(train_prop*len(df))\n","print (train_len)\n","# ---- Train / Test\n","dataset_train = df.loc[ :train_len-1, features ]\n","dataset_test  = df.loc[train_len:,    features ]\n","pwk.subtitle('Train dataset example :')\n","display(dataset_train.head(31))\n","\n","# ---- Normalize, and convert to numpy array\n","\n","mean = dataset_train.mean()\n","std  = dataset_train.std()\n","dataset_train = (dataset_train - mean) / std\n","dataset_test  = (dataset_test  - mean) / std\n","\n","pwk.subtitle('After normalization :')\n","display(dataset_train.describe().style.format(\"{0:.2f}\"))\n","\n","dataset_train = dataset_train.to_numpy()\n","dataset_test  = dataset_test.to_numpy()\n","\n","pwk.subtitle('Shapes :')\n","print('Dataset       : ',df.shape)\n","print('Train dataset : ',dataset_train.shape)\n","print('Test  dataset : ',dataset_test.shape)"]},{"cell_type":"code","execution_count":null,"id":"efe03096","metadata":{"id":"efe03096"},"outputs":[],"source":["# ---- Train generator\n","\n","train_generator = TimeseriesGenerator(dataset_train, dataset_train, length=sequence_len,  batch_size=batch_size)\n","test_generator  = TimeseriesGenerator(dataset_test,  dataset_test,  length=sequence_len,  batch_size=batch_size)\n","\n","# ---- About\n","\n","pwk.subtitle('About the splitting of our dataset :')\n","\n","x,y=train_generator[0]\n","print(f'Nombre de train batchs disponibles : ', len(train_generator))\n","print('batch x shape : ',x.shape)\n","print('batch y shape : ',y.shape)\n","\n","x,y=train_generator[0]\n","pwk.subtitle('What a batch looks like (x[0]) :')\n","pwk.np_print(x[0] )\n","pwk.subtitle('What a batch looks like (y[0]) :')\n","pwk.np_print(y[0])"]},{"cell_type":"code","execution_count":null,"id":"6199b450","metadata":{"id":"6199b450"},"outputs":[],"source":["model = keras.models.Sequential()\n","model.add( keras.layers.InputLayer(input_shape=(sequence_len, features_len)) )\n","model.add( keras.layers.LSTM(100, activation='relu') ) #bon ca doit pas etre normal\n","model.add( keras.layers.Dropout(0.2) )\n","model.add( keras.layers.Dense(features_len) )\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"id":"38b456ff","metadata":{"id":"38b456ff"},"outputs":[],"source":["pwk.mkdir(run_dir)\n","save_dir = f'{run_dir}/best_model.h10'\n","bestmodel_callback = tf.keras.callbacks.ModelCheckpoint(filepath=save_dir, verbose=0, save_best_only=True)"]},{"cell_type":"code","execution_count":null,"id":"b8c4bb0c","metadata":{"id":"b8c4bb0c"},"outputs":[],"source":[" model.compile(optimizer='adam', \n","              loss='mse', \n","              metrics   = ['mae'] )"]},{"cell_type":"code","execution_count":null,"id":"33aedda7","metadata":{"id":"33aedda7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654767118984,"user_tz":-120,"elapsed":187724,"user":{"displayName":"Guillaume Principato","userId":"02696761146641071970"}},"outputId":"8f6b89f4-d4b5-4da1-bb67-27d7662e0788"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","424/424 [==============================] - 19s 39ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n","Epoch 2/10\n","424/424 [==============================] - 16s 37ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n","Epoch 3/10\n","424/424 [==============================] - 16s 37ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n","Epoch 4/10\n","424/424 [==============================] - 16s 38ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n","Epoch 5/10\n","424/424 [==============================] - 19s 45ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n","Epoch 6/10\n","424/424 [==============================] - 16s 38ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n","Epoch 7/10\n","424/424 [==============================] - 16s 37ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n","Epoch 8/10\n","424/424 [==============================] - 16s 37ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n","Epoch 9/10\n","424/424 [==============================] - 16s 38ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n","Epoch 10/10\n","424/424 [==============================] - 15s 36ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n","\n","Duration :  00:03:08 508ms\n"]}],"source":[" pwk.chrono_start()\n","\n","history=model.fit(train_generator, \n","                  epochs  = epochs, \n","                  verbose = fit_verbosity,\n","                  validation_data = test_generator,\n","                  callbacks = [bestmodel_callback])\n","\n","pwk.chrono_show()"]},{"cell_type":"code","execution_count":null,"id":"8706e856","metadata":{"id":"8706e856"},"outputs":[],"source":[" pwk.plot_history(history,plot={'loss':['loss','val_loss'], 'mae':['mae','val_mae']}, save_as='01-history') "]},{"cell_type":"markdown","source":[""],"metadata":{"id":"e4MhYGinXfFV"},"id":"e4MhYGinXfFV"},{"cell_type":"code","execution_count":null,"id":"32beff8d","metadata":{"id":"32beff8d"},"outputs":[],"source":[" loaded_model1 = tf.keras.models.load_model(f'{run_dir}/best_model.h5')"]},{"cell_type":"markdown","source":["A partir de là, on double tout le code pour cette fois faire le modele uniquement sur les données d'une ville\n"],"metadata":{"id":"nT5WGUCZXf5L"},"id":"nT5WGUCZXf5L"},{"cell_type":"code","source":["dataset_filename = 'synop-LYSinterpole.csv'\n","schema_filename  = 'synopinterpole.json'\n","features         = ['Température','mois_de_l_annee','Humidité']\n","features_len     = len(features)\n","\n","# ---- About training (Can be changed !)\n","#\n","scale            = 1        # Percentage of dataset to be used (1=all)\n","train_prop       = .8       # Percentage for train (the rest being for the test)\n","sequence_len     = 32\n","batch_size       = 64\n","epochs           = 10\n","fit_verbosity    = 1        # 0 = silent, 1 = progress bar, 2 = one line per epoch"],"metadata":{"id":"xkpYzLH3X0to"},"id":"xkpYzLH3X0to","execution_count":null,"outputs":[]},{"cell_type":"code","source":[" pwk.override('scale', 'train_prop', 'sequence_len', 'batch_size', 'epochs', 'fit_verbosity')"],"metadata":{"id":"bc8HaQZaX0r3"},"id":"bc8HaQZaX0r3","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---- Read dataset from ./data\n","\n","df = pd.read_csv(f'{dataset_dir}/{dataset_filename}', header=0, sep=';')\n","df1 = pd.DataFrame( df.iloc[0])\n","# ---- Scaling\n","\n","for i in range (0,len(df),3):\n","  df1[i//3]=(df.iloc[i])\n","df1 = df1[:int(scale*len(df1))]\n","df1=df1.T\n","train_len1=int(train_prop*len(df1))\n","\n","\n","# ---- Train / Test\n","dataset_train1 = df1.loc[ :train_len1-1, features ]\n","dataset_test1  = df1.loc[train_len1:,    features ]\n","pwk.subtitle('Train dataset example :')\n","display(dataset_train1.head(31))\n","\n","# ---- Normalize, and convert to numpy array\n","\n","mean1 = dataset_train1.mean()\n","std1  = dataset_train1.std()\n","dataset_train1 = (dataset_train1 - mean1) / std1\n","dataset_test1  = (dataset_test1  - mean1) / std1\n","\n","pwk.subtitle('After normalization :')\n","display(dataset_train1.describe().style.format(\"{0:.2f}\"))\n","\n","dataset_train1 = dataset_train1.to_numpy()\n","dataset_test1  = dataset_test1.to_numpy()\n","\n","pwk.subtitle('Shapes :')\n","print('Dataset       : ',df1.shape)\n","print('Train dataset : ',dataset_train1.shape)\n","print('Test  dataset : ',dataset_test1.shape)"],"metadata":{"id":"oc_jg2pLX0pR"},"id":"oc_jg2pLX0pR","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---- Train generator\n","sequence_len1=sequence_len//3\n","batch_size1=2*sequence_len1\n","\n","#sequence_len1=sequence_len\n","#batch_size1=batch_size\n","\n","train_generator1 = TimeseriesGenerator(dataset_train1, dataset_train1, length=sequence_len1,  batch_size=batch_size1)\n","test_generator1  = TimeseriesGenerator(dataset_test1,  dataset_test1,  length=sequence_len1,  batch_size=batch_size1)\n","\n","# ---- About\n","\n","pwk.subtitle('About the splitting of our dataset :')\n","\n","x,y=train_generator1[0]\n","print(f'Nombre de train batchs disponibles : ', len(train_generator1))\n","print('batch x shape : ',x.shape)\n","print('batch y shape : ',y.shape)\n","\n","x,y=train_generator1[0]\n","pwk.subtitle('What a batch looks like (x[0]) :')\n","pwk.np_print(x[0] )\n","pwk.subtitle('What a batch looks like (y[0]) :')\n","pwk.np_print(y[0])\n","print (batch_size1)"],"metadata":{"id":"MAXwHP6PX0mn"},"id":"MAXwHP6PX0mn","execution_count":null,"outputs":[]},{"cell_type":"code","source":["model1 = keras.models.Sequential()\n","model1.add( keras.layers.InputLayer(input_shape=(sequence_len1, features_len)) )\n","model1.add( keras.layers.LSTM(100, activation='relu') ) #bon ca doit pas etre normal\n","model1.add( keras.layers.Dropout(0.2) )\n","model1.add( keras.layers.Dense(features_len) )\n","\n","model1.summary()"],"metadata":{"id":"5Qn0x06-X0kF"},"id":"5Qn0x06-X0kF","execution_count":null,"outputs":[]},{"cell_type":"code","source":["pwk.mkdir(run_dir)\n","save_dir1 = f'{run_dir}/best_model1.h5'\n","bestmodel_callback1 = tf.keras.callbacks.ModelCheckpoint(filepath=save_dir1, verbose=0, save_best_only=True)\n","print(bestmodel_callback1)\n"],"metadata":{"id":"2CfFg6kQX0hk"},"id":"2CfFg6kQX0hk","execution_count":null,"outputs":[]},{"cell_type":"code","source":[" model1.compile(optimizer='adam', \n","              loss='mse', \n","              metrics   = ['mae'] )"],"metadata":{"id":"UoG3QkF8swMs"},"id":"UoG3QkF8swMs","execution_count":null,"outputs":[]},{"cell_type":"code","source":[" pwk.chrono_start()\n","\n","history1=model1.fit(train_generator1, \n","                  epochs  = epochs, \n","                  verbose = fit_verbosity,\n","                  validation_data = test_generator1,\n","                  callbacks = [bestmodel_callback1])\n","\n","pwk.chrono_show()"],"metadata":{"id":"hUd58nOsswEe"},"id":"hUd58nOsswEe","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"XBYejSGUX0ei"},"id":"XBYejSGUX0ei","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"pkhE2IO-X0UM"},"id":"pkhE2IO-X0UM","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"fc1665af","metadata":{"id":"fc1665af"},"outputs":[],"source":["s1=random.randint(0,(len(dataset_test)-sequence_len)//3)\n","s=3*s1      #comme ca on peut décider où on commence pour la séquence (ici à Barberey)\n","#sequence      = dataset_test[i in range(s,s+sequence_len,1)]\n","sequence      = dataset_test[s:s+sequence_len]\n","#sequence_true = dataset_test[s:s+sequence_len+1]\n","sequence2=[]\n","sequence_true2=[]\n","for i in range (s,s+sequence_len,1):\n","  sequence2.append(dataset_test[i])\n","for i in range (s,s+sequence_len+3,3):\n","  sequence_true2.append(dataset_test[i])\n","\n","pred = loaded_model1.predict( np.array([sequence2]) )\n","\n","# ---- Show result\n","pwk.plot_multivariate_serie(np.array(sequence_true2), predictions=pred, labels=features, save_as='02-prediction-norm')"]},{"cell_type":"code","execution_count":null,"id":"7c9d6470","metadata":{"id":"7c9d6470"},"outputs":[],"source":["def denormalize(mean,std,seq):\n","    nseq = seq.copy()\n","    for i,s in enumerate(nseq):\n","        s = s*std + mean\n","        nseq[i]=s\n","    return nseq\n","\n","\n","# ---- Get a sequence\n","s1=random.randint(0,(len(dataset_test)-sequence_len)//3)\n","s=3*s1      #comme ca on peut décider où on commence pour la séquence (ici à Barberey)\n","#sequence      = dataset_test[i:i+sequence_len]\n","#sequence_true = dataset_test[i:i+sequence_len+1]\n","sequence2=[]\n","sequence_true2=[]\n","for i in range (s,s+sequence_len,1):\n","  sequence2.append(dataset_test[i])\n","for i in range (s,s+sequence_len+3,3):\n","  sequence_true2.append(dataset_test[i])\n","\n","\n","\n","\n","# ---- Prediction\n","print (np.array([sequence2]).shape)\n","pred = loaded_model1.predict( np.array([sequence2]) )\n","\n","\n","# ---- De-normalization\n","\n","sequence_true = denormalize(mean,std, np.array(sequence_true2))\n","pred          = denormalize(mean,std, pred)\n","\n","# ---- Show it\n","feat=0 #la feature \"température\"\n","feat2=2 #la feature 'humidité\"\n","\n","pwk.plot_multivariate_serie(sequence_true, predictions=pred, labels=features, only_features=[feat],width=14, height=8, save_as='03-prediction')\n","\n","delta_deg=abs(sequence_true[-1][feat]-pred[-1][feat])\n","\n","pwk.plot_multivariate_serie(sequence_true, predictions=pred, labels=features, only_features=[feat2],width=14, height=8, save_as='03-prediction')\n","\n","delta_deg=abs(sequence_true[-1][feat2]-pred[-1][feat])\n","print(f'Gap between prediction and reality : {delta_deg:.2f} °C')"]},{"cell_type":"code","execution_count":null,"id":"67f2d43f","metadata":{"id":"67f2d43f"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"id":"267a4b2c","metadata":{"id":"267a4b2c"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"colab":{"name":"entraînement interpolé 1.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":5}